# Enterprise Data Pipeline

Pipeline de dados enterprise-grade usando **AWS Kinesis + Apache Airflow + DBT** para processamento em tempo real e analytics avan√ßadas.

## üèóÔ∏è Arquitetura

```
Kinesis Streams ‚Üí Bronze Layer ‚Üí Silver Layer ‚Üí Gold Layer ‚Üí Analytics
     (Raw)         (Airflow)     (Airflow)      (DBT)      (Grafana)
```

### Camadas de Dados

- **Bronze (Raw)**: Dados brutos do Kinesis ‚Üí PostgreSQL
- **Silver (Clean)**: Dados limpos e enriquecidos via Airflow
- **Gold (Business)**: M√©tricas de neg√≥cio e analytics via DBT

## üöÄ Caracter√≠sticas Enterprise

- **Real-time Ingestion**: Kinesis Streams para dados em tempo real
- **Orquestra√ß√£o Robusta**: Airflow com retry, monitoring e alertas
- **Data Quality**: Valida√ß√µes autom√°ticas em cada camada
- **Analytics Engineering**: DBT para transforma√ß√µes SQL complexas
- **Observabilidade**: Grafana + m√©tricas customizadas
- **Escalabilidade**: Arquitetura distribu√≠da e containerizada

## üõ†Ô∏è Stack Tecnol√≥gica

### Ingest√£o de Dados
- **AWS Kinesis**: Streaming de dados em tempo real
- **LocalStack**: Simula√ß√£o AWS local para desenvolvimento
- **Python**: Produtores de dados enterprise

### Orquestra√ß√£o
- **Apache Airflow**: Workflow orchestration
- **PostgreSQL**: Data warehouse e metastore
- **Redis**: Celery backend para Airflow

### Transforma√ß√£o
- **DBT**: Analytics engineering e data modeling
- **SQL**: Transforma√ß√µes complexas e business logic
- **Jinja2**: Templates din√¢micos no DBT

### Monitoramento
- **Grafana**: Dashboards e visualiza√ß√µes
- **Airflow UI**: Monitoring de pipelines
- **DBT Docs**: Documenta√ß√£o autom√°tica

## üìä Dados Simulados

### Streams Kinesis
1. **customer-events**: Cadastros, logins, atualiza√ß√µes
2. **transaction-events**: Transa√ß√µes financeiras completas
3. **product-events**: Intera√ß√µes com produtos
4. **system-logs**: Logs de aplica√ß√µes e servi√ßos

### M√©tricas Calculadas
- **Customer Lifetime Value (LTV)**
- **Risk Scoring** com ML features
- **RFM Analysis** (Recency, Frequency, Monetary)
- **Cohort Analysis** e reten√ß√£o
- **Revenue Attribution** por canal/categoria

## üöÄ Como Executar

### 1. Preparar Ambiente
```bash
# Criar diret√≥rios necess√°rios
mkdir -p airflow/{dags,logs,plugins}
echo -e "AIRFLOW_UID=$(id -u)" > .env

# Subir infraestrutura
docker-compose up -d

# Aguardar inicializa√ß√£o (3-5 minutos)
docker-compose logs -f airflow-init
```

### 2. Configurar Conex√µes Airflow
```bash
# Acessar Airflow UI: http://localhost:8080 (airflow/airflow)
# Configurar conex√£o PostgreSQL:
# - Conn Id: postgres_default
# - Host: postgres
# - Schema: airflow
# - Login: airflow
# - Password: airflow
```

### 3. Iniciar Pipeline
```bash
# Terminal 1: Gerar dados Kinesis
cd kinesis
pip install boto3 faker
python data_producer.py

# Terminal 2: Ativar DAG no Airflow UI
# Ou via CLI:
docker exec airflow-scheduler airflow dags unpause enterprise_data_pipeline
```

### 4. Executar DBT
```bash
# Dentro do container Airflow
docker exec -it airflow-scheduler bash
cd /opt/airflow/dbt

# Instalar depend√™ncias e executar
dbt deps
dbt run
dbt test
dbt docs generate
```

## üìà Modelos de Dados

### Bronze Layer
- `bronze_customer_events`: Eventos brutos de clientes
- `bronze_transaction_events`: Transa√ß√µes brutas
- `bronze_product_events`: Intera√ß√µes com produtos
- `bronze_system_logs`: Logs de sistema

### Silver Layer
- `silver_customers`: Clientes consolidados e enriquecidos
- `silver_transactions`: Transa√ß√µes limpas com categoriza√ß√£o
- `silver_products`: Cat√°logo de produtos normalizado
- `silver_sessions`: Sess√µes de usu√°rio agregadas

### Gold Layer
- `gold_customer_analytics`: Analytics 360¬∞ de clientes
- `gold_business_metrics`: KPIs e m√©tricas de neg√≥cio
- `gold_cohort_analysis`: An√°lise de coortes
- `gold_revenue_attribution`: Atribui√ß√£o de receita

## üîç Qualidade de Dados

### Valida√ß√µes Autom√°ticas
- **Completude**: Campos obrigat√≥rios preenchidos
- **Unicidade**: Chaves prim√°rias √∫nicas
- **Consist√™ncia**: Relacionamentos v√°lidos
- **Freshness**: Dados atualizados nas √∫ltimas 2 horas
- **Volume**: Varia√ß√£o de volume dentro do esperado

### Testes DBT
```sql
-- Exemplo de teste customizado
SELECT COUNT(*) as failed_records
FROM {{ ref('silver_customers') }}
WHERE email NOT LIKE '%@%'
  OR total_spent < 0
  OR created_at > CURRENT_TIMESTAMP
```

## üìä Dashboards e M√©tricas

### KPIs Principais
- **Revenue Growth**: Crescimento de receita MoM/YoY
- **Customer Acquisition Cost (CAC)**
- **Monthly Recurring Revenue (MRR)**
- **Churn Rate** por segmento
- **Average Order Value (AOV)**

### Dashboards Grafana
- **Executive Dashboard**: KPIs executivos
- **Operations Dashboard**: M√©tricas operacionais
- **Data Quality Dashboard**: Sa√∫de dos dados
- **Pipeline Monitoring**: Status dos jobs

## üîß Configura√ß√µes Avan√ßadas

### Airflow
```python
# airflow.cfg customizations
[core]
executor = LocalExecutor
max_active_runs_per_dag = 1
dagbag_import_timeout = 30

[scheduler]
catchup_by_default = False
max_threads = 4
```

### DBT
```yaml
# dbt_project.yml
models:
  enterprise_dw:
    bronze:
      +materialized: table
      +pre-hook: "{{ log('Processing bronze layer', info=True) }}"
    silver:
      +materialized: table
      +post-hook: "{{ run_query('ANALYZE ' ~ this) }}"
    gold:
      +materialized: table
      +indexes:
        - columns: ['customer_id']
          unique: true
```

## üß™ Testes e Valida√ß√£o

### Testes Unit√°rios
```bash
# Testar modelos DBT
dbt test --models silver_customers
dbt test --models gold_customer_analytics

# Testar DAGs Airflow
python -m pytest tests/test_dags.py
```

### Testes de Integra√ß√£o
```bash
# Validar pipeline end-to-end
./scripts/integration_test.sh
```

## üöÄ Deploy e CI/CD

### Ambientes
- **DEV**: Desenvolvimento local com LocalStack
- **STAGING**: Ambiente de homologa√ß√£o
- **PROD**: Produ√ß√£o com AWS real

### Pipeline CI/CD
```yaml
# .github/workflows/deploy.yml
- name: Test DBT Models
  run: dbt test --profiles-dir ./profiles

- name: Deploy to Staging
  run: dbt run --target staging

- name: Run Data Quality Checks
  run: python scripts/data_quality_check.py
```

## üìö Tecnologias Demonstradas

### Data Engineering
- **Stream Processing**: Kinesis real-time ingestion
- **Batch Processing**: Airflow orchestration
- **Data Modeling**: DBT dimensional modeling
- **Data Quality**: Automated testing and validation

### DevOps & MLOps
- **Containerization**: Docker multi-service setup
- **Infrastructure as Code**: Docker Compose
- **Monitoring**: Grafana + custom metrics
- **Documentation**: Auto-generated DBT docs

### Analytics Engineering
- **Dimensional Modeling**: Star schema implementation
- **Business Logic**: Complex SQL transformations
- **Performance Optimization**: Indexes and partitioning
- **Data Lineage**: DBT automatic lineage tracking

## üîÑ Pr√≥ximos Passos

- [ ] Implementar Apache Iceberg para data lakehouse
- [ ] Adicionar Great Expectations para data quality
- [ ] Integrar com Apache Superset para self-service BI
- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar machine learning features
- [ ] Configurar alertas inteligentes

---

**Desenvolvido por Ivan de Fran√ßa**

*Pipeline enterprise demonstrando as melhores pr√°ticas em Data Engineering, Analytics Engineering e DataOps.*